# Sequence to Sequence Model with Attention
This is a sequence to sequence model with attention. This is just to demonstrate the architecture for performance comparision you must:

- Get a large training data sets
- Clean the data set properly to limit the vocabulary size
- Tune the hyper-parameters



References :
- [Attention Mechanisms in Recurrent Neural Networks (RNNs) With Keras)](Attention Mechanisms in Recurrent Neural Networks (RNNs) With Keras)
- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

