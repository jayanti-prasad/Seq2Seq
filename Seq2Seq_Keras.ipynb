{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec2ec307-fc16-4bd6-ba30-4486a2df3ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Seq2Seq-Model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_97 (InputLayer)       [(None, None, 20)]           0         []                            \n",
      "                                                                                                  \n",
      " input_98 (InputLayer)       [(None, None, 10)]           0         []                            \n",
      "                                                                                                  \n",
      " LSTM-1 (LSTM)               [(None, 60),                 19440     ['input_97[0][0]']            \n",
      "                              (None, 60),                                                         \n",
      "                              (None, 60)]                                                         \n",
      "                                                                                                  \n",
      " LSTM-2 (LSTM)               [(None, None, 60),           17040     ['input_98[0][0]',            \n",
      "                              (None, 60),                            'LSTM-1[0][1]',              \n",
      "                              (None, 60)]                            'LSTM-1[0][2]']              \n",
      "                                                                                                  \n",
      " dense_25 (Dense)            (None, None, 10)             610       ['LSTM-2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 37090 (144.88 KB)\n",
      "Trainable params: 37090 (144.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"lstm_3\" (type LSTM).\n\nDimensions must be equal, but are 10 and 2000 for '{{node MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](strided_slice_1, kernel)' with input shapes: [?,10], [2000,240].\n\nCall arguments received by layer \"lstm_3\" (type LSTM):\n  • inputs=['tf.Tensor(shape=(None, None, 10), dtype=float32)', 'tf.Tensor(shape=(None, 60), dtype=float32)', 'tf.Tensor(shape=(None, 60), dtype=float32)']\n  • mask=None\n  • training=None\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 112\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded_sentence\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 112\u001b[0m      S \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2Seq_Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_encoder_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_decoder_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m      \u001b[38;5;28mprint\u001b[39m(S\u001b[38;5;241m.\u001b[39mencoder_model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m    115\u001b[0m      \u001b[38;5;28mprint\u001b[39m(S\u001b[38;5;241m.\u001b[39mdecoder_model\u001b[38;5;241m.\u001b[39msummary())\n",
      "Cell \u001b[1;32mIn[47], line 12\u001b[0m, in \u001b[0;36mSeq2Seq_Model.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_dim\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_model() \n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_encoder_decoder_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 49\u001b[0m, in \u001b[0;36mSeq2Seq_Model.build_encoder_decoder_models\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m decoder_state_input_c \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim,))\n\u001b[0;32m     48\u001b[0m decoder_states_inputs \u001b[38;5;241m=\u001b[39m [decoder_state_input_h, decoder_state_input_c]\n\u001b[1;32m---> 49\u001b[0m decoder_outputs, state_h, state_c \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_states_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m decoder_states \u001b[38;5;241m=\u001b[39m [state_h, state_c]\n\u001b[0;32m     52\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_dense(decoder_outputs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\aiml_env\\lib\\site-packages\\keras\\src\\layers\\rnn\\base_rnn.py:615\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;66;03m# Perform the call with temporarily replaced input_spec\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_spec \u001b[38;5;241m=\u001b[39m full_input_spec\n\u001b[1;32m--> 615\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(full_input, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# Remove the additional_specs from input spec and keep the rest. It\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# is important to keep since the input spec was populated by\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;66;03m# build(), and will be reused in the stateful=True.\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_spec[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(additional_specs)]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\aiml_env\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\aiml_env\\lib\\site-packages\\keras\\src\\backend.py:2465\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   2463\u001b[0m     out \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39msparse_dense_matmul(x, y)\n\u001b[0;32m   2464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2465\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"lstm_3\" (type LSTM).\n\nDimensions must be equal, but are 10 and 2000 for '{{node MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](strided_slice_1, kernel)' with input shapes: [?,10], [2000,240].\n\nCall arguments received by layer \"lstm_3\" (type LSTM):\n  • inputs=['tf.Tensor(shape=(None, None, 10), dtype=float32)', 'tf.Tensor(shape=(None, 60), dtype=float32)', 'tf.Tensor(shape=(None, 60), dtype=float32)']\n  • mask=None\n  • training=None\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "\n",
    "\n",
    "class Seq2Seq_Model:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.num_encoder_tokens = kwargs ['num_encoder_tokens']\n",
    "        self.num_decoder_tokens = kwargs ['num_decoder_tokens']\n",
    "        self.latent_dim = kwargs['latent_dim']\n",
    "        self.build_model() \n",
    "        self.build_encoder_decoder_models()\n",
    "        \n",
    "    \n",
    "    def build_model (self, ):\n",
    "        # Define an input sequence and process it.\n",
    "        self.encoder_inputs = Input(shape=(None, self.num_encoder_tokens))\n",
    "        encoder = LSTM(self.latent_dim, return_state=True,name='LSTM-1')\n",
    "        encoder_outputs, state_h, state_c = encoder(self.encoder_inputs)\n",
    "        # We discard `encoder_outputs` and only keep the states.\n",
    "        self.encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "        # Set up the decoder, using `encoder_states` as initial state.\n",
    "        self.decoder_inputs = Input(shape=(None, self.num_decoder_tokens))\n",
    "        # We set up our decoder to return full output sequences,\n",
    "        # and to return internal states as well. We don't use the \n",
    "\n",
    "        # return states in the training model, but we will use them in inference.\n",
    "        decoder_lstm = LSTM(self.latent_dim, return_sequences=True, return_state=True,name='LSTM-2')\n",
    "        decoder_outputs, _, _ = decoder_lstm(self.decoder_inputs,\n",
    "                                     initial_state=self.encoder_states)\n",
    "        self.decoder_dense = Dense(self.num_decoder_tokens, activation='softmax')\n",
    "        decoder_outputs = self.decoder_dense(decoder_outputs)\n",
    "\n",
    "        # Define the model that will turn\n",
    "        # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "        self.model = Model([self.encoder_inputs, self.decoder_inputs], decoder_outputs, name='Seq2Seq-Model')\n",
    "        \n",
    "        print(self.model.summary())\n",
    "\n",
    "    def build_encoder_decoder_models (self,):\n",
    "        self.encoder_model = Model(self.encoder_inputs, self.encoder_states,name='Encoder-Model')\n",
    "\n",
    "        decoder_state_input_h = Input(shape=(self.latent_dim,))\n",
    "        decoder_state_input_c = Input(shape=(self.latent_dim,))\n",
    "\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(self.decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = self.decoder_dense(decoder_outputs)\n",
    "\n",
    "        self.decoder_model = Model([self.decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states, name='Decoder-Model')\n",
    "       \n",
    "       \n",
    "\n",
    "    \n",
    "    def train_model (self, X, y, batch_size, epochs):\n",
    "        encoder_input_data = X\n",
    "        decoder_input_data = Y[:, :-1]\n",
    "        decoder_target_data = Y[:, 1:]\n",
    "        \n",
    "\n",
    "        # Run training\n",
    "        self.model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "        hist = self.model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "        return hist \n",
    "        \n",
    " \n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "     S = Seq2Seq_Model(num_encoder_tokens = 20, num_decoder_tokens = 10, latent_dim = 60)\n",
    "\n",
    "     print(S.encoder_model.summary())\n",
    "     print(S.decoder_model.summary())\n",
    "\n",
    "     num_encoder_tokens = 1000\n",
    "     num_decoder_tokens = 2000\n",
    "\n",
    "     #X  = np.random.randint (0, num_encoder_tokens-1, size = (1000, 20))\n",
    "     #Y  = np.random.randint (0, num_encoder_tokens-1, size = (1000, 30))\n",
    "\n",
    "     X = np.random.random ([1000,20])\n",
    "     Y = np.random.random ([1000,10])\n",
    "    \n",
    "\n",
    "    \n",
    "     print(\"Input data\", X.shape, Y.shape)\n",
    "    \n",
    "     hist = S.train_model (X, Y, 60, 60)\n",
    "     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ad000-42c4-48eb-bfb2-7f74b5ecd691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
